{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning ESM2\n",
    "\n",
    "requirements: transformers, datasets, peft, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # log into huggingface for pushing the model to model hub\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['protein_id', 'seq_len', 'sequence', 'label'],\n",
       "        num_rows: 10368\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['protein_id', 'seq_len', 'sequence', 'label'],\n",
       "        num_rows: 1296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['protein_id', 'seq_len', 'sequence', 'label'],\n",
       "        num_rows: 1297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# add paths to datasets on your machine\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"tox_train.tsv\", \n",
    "                                          \"validation\":\"tox_validation.tsv\",\n",
    "                                          \"test\":\"tox_test.tsv\"}, delimiter=\"\\t\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'protein_id': Value(dtype='string', id=None),\n",
       " 'seq_len': Value(dtype='int64', id=None),\n",
       " 'sequence': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['train'][0]\n",
    "dataset['train'].features\n",
    "# dataset.save_to_disk(\"datasetdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same tokenizer that was used for pre-training\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sequence']) # leave out padding=True, pad dynamically when batches are created\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True) # batched -> speedup tokenization\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2, problem_type='single_label_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> R F R F R V K C S K G T Y <eos>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10368\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_dataset['train'][0]\n",
    "# Columns to keep\n",
    "keep_columns = ['input_ids', 'attention_mask', 'label']\n",
    "\n",
    "for split in tokenized_dataset:\n",
    "    tokenized_dataset[split] = tokenized_dataset[split] \\\n",
    "        .remove_columns([col for col in tokenized_dataset[split].column_names if col not in keep_columns])\n",
    "\n",
    "for split in tokenized_dataset:\n",
    "    # Rename the 'label' column to 'labels'\n",
    "    tokenized_dataset[split] = tokenized_dataset[split].rename_column('label', 'labels')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic padding\n",
    "# from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# for step, batch in enumerate(train_dataloader):\n",
    "#     print(batch['input_ids'].shape)\n",
    "#     if step > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmConfig {\n",
      "  \"_name_or_path\": \"facebook/esm2_t33_650M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5120,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"esm\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 33,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.esm.configuration_esm.EsmConfig"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "print(config)\n",
    "type(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.esm.modeling_esm.EsmForSequenceClassification'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2, problem_type=\"single_label_classification\")\n",
    "print(type(model))\n",
    "print(isinstance(model, torch.nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the lm_head) and adding some weights for sequence classification (the classifier). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEQ_CLS',\n",
       " 'SEQ_2_SEQ_LM',\n",
       " 'CAUSAL_LM',\n",
       " 'TOKEN_CLS',\n",
       " 'QUESTION_ANS',\n",
       " 'FEATURE_EXTRACTION']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    inference_mode=False,\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    # target_modules=[\"query\",\n",
    "    #                 \"key\",\n",
    "    #                 \"value\"]\n",
    "    target_modules=\"all-linear\"\n",
    "    )\n",
    "[e.value for e in TaskType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,879,410 || all params: 666,297,385 || trainable%: 2.0831\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric for evaluation\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "# metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Calculate confusion matrix values: TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    # Calculate other metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, predictions)\n",
    "    except ValueError:\n",
    "        auc = \"N/A\"  # Handle the case when there are no positive labels\n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    # Create a pandas dataframe to store the results\n",
    "    metrics = {\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"MCC\": mcc,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/esm_env/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Trainer arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned_toxi\",\n",
    "    # evaluation_strategy = \"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    save_strategy =\"steps\",\n",
    "    save_steps=400,\n",
    "    # save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    # learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"MCC\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of callback for tracking memory\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class CUDAMemoryLogger(TrainerCallback):\n",
    "    def __init__(self, log_every_n_steps=100):\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model, tokenizer, logs=None, **kwargs):\n",
    "        # Log CUDA memory usage every 'log_every_n_steps' steps\n",
    "        if state.global_step % self.log_every_n_steps == 0:\n",
    "            allocated_memory = torch.cuda.memory_allocated() / 1024**2  # in MB\n",
    "            reserved_memory = torch.cuda.memory_reserved() / 1024**2  # in MB\n",
    "            print(f\"Step {state.global_step}:\")\n",
    "            print(f\"  Allocated memory: {allocated_memory:.2f} MB\")\n",
    "            print(f\"  Reserved memory: {reserved_memory:.2f} MB\")\n",
    "            \n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator, # DataCollatorWithPadding is the default in Trainer, but we specified it\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CUDAMemoryLogger(log_every_n_steps=400)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq='MIDYVGSFLGAYFLGFALFYGIGFFKSISNRIIIGI'\n",
    "# input = tokenizer(seq, return_tensors='pt')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# input['labels'] = torch.tensor(0)\n",
    "# out = model(**input.to(device))\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolamilicevic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/finetuning/wandb/run-20241202_185700-up2hsf2z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolamilicevic/huggingface/runs/up2hsf2z' target=\"_blank\">esm2_t33_650M_UR50D-finetuned_toxi</a></strong> to <a href='https://wandb.ai/nikolamilicevic/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolamilicevic/huggingface' target=\"_blank\">https://wandb.ai/nikolamilicevic/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolamilicevic/huggingface/runs/up2hsf2z' target=\"_blank\">https://wandb.ai/nikolamilicevic/huggingface/runs/up2hsf2z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2592' max='2592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2592/2592 16:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Tn</th>\n",
       "      <th>Fp</th>\n",
       "      <th>Fn</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.382058</td>\n",
       "      <td>506</td>\n",
       "      <td>578</td>\n",
       "      <td>86</td>\n",
       "      <td>126</td>\n",
       "      <td>0.836420</td>\n",
       "      <td>0.854730</td>\n",
       "      <td>0.800633</td>\n",
       "      <td>0.826797</td>\n",
       "      <td>0.835557</td>\n",
       "      <td>0.673430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.358143</td>\n",
       "      <td>531</td>\n",
       "      <td>572</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>0.851080</td>\n",
       "      <td>0.852327</td>\n",
       "      <td>0.840190</td>\n",
       "      <td>0.846215</td>\n",
       "      <td>0.850818</td>\n",
       "      <td>0.701944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.350189</td>\n",
       "      <td>512</td>\n",
       "      <td>591</td>\n",
       "      <td>73</td>\n",
       "      <td>120</td>\n",
       "      <td>0.851080</td>\n",
       "      <td>0.875214</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.841413</td>\n",
       "      <td>0.850093</td>\n",
       "      <td>0.703305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.349700</td>\n",
       "      <td>0.343169</td>\n",
       "      <td>522</td>\n",
       "      <td>585</td>\n",
       "      <td>79</td>\n",
       "      <td>110</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.868552</td>\n",
       "      <td>0.825949</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.853487</td>\n",
       "      <td>0.708624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.334348</td>\n",
       "      <td>524</td>\n",
       "      <td>582</td>\n",
       "      <td>82</td>\n",
       "      <td>108</td>\n",
       "      <td>0.853395</td>\n",
       "      <td>0.864686</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>0.846527</td>\n",
       "      <td>0.852810</td>\n",
       "      <td>0.706891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.334452</td>\n",
       "      <td>514</td>\n",
       "      <td>596</td>\n",
       "      <td>68</td>\n",
       "      <td>118</td>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.883162</td>\n",
       "      <td>0.813291</td>\n",
       "      <td>0.846787</td>\n",
       "      <td>0.855441</td>\n",
       "      <td>0.714380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400:\n",
      "  Allocated memory: 2767.32 MB\n",
      "  Reserved memory: 16458.00 MB\n",
      "Step 800:\n",
      "  Allocated memory: 2767.06 MB\n",
      "  Reserved memory: 16458.00 MB\n",
      "Step 1200:\n",
      "  Allocated memory: 2767.06 MB\n",
      "  Reserved memory: 16458.00 MB\n",
      "Step 1600:\n",
      "  Allocated memory: 2766.99 MB\n",
      "  Reserved memory: 16458.00 MB\n",
      "Step 2000:\n",
      "  Allocated memory: 2767.09 MB\n",
      "  Reserved memory: 16458.00 MB\n",
      "Step 2400:\n",
      "  Allocated memory: 2767.06 MB\n",
      "  Reserved memory: 16458.00 MB\n"
     ]
    }
   ],
   "source": [
    "# LoRA\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolamilicevic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/finetuning/wandb/run-20241202_192622-8pb7zn47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolamilicevic/huggingface/runs/8pb7zn47' target=\"_blank\">esm2_t33_650M_UR50D-finetuned_toxi</a></strong> to <a href='https://wandb.ai/nikolamilicevic/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolamilicevic/huggingface' target=\"_blank\">https://wandb.ai/nikolamilicevic/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolamilicevic/huggingface/runs/8pb7zn47' target=\"_blank\">https://wandb.ai/nikolamilicevic/huggingface/runs/8pb7zn47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2592' max='2592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2592/2592 24:54, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Tn</th>\n",
       "      <th>Fp</th>\n",
       "      <th>Fn</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.337577</td>\n",
       "      <td>541</td>\n",
       "      <td>563</td>\n",
       "      <td>101</td>\n",
       "      <td>91</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.842679</td>\n",
       "      <td>0.856013</td>\n",
       "      <td>0.849294</td>\n",
       "      <td>0.851952</td>\n",
       "      <td>0.703720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.309057</td>\n",
       "      <td>547</td>\n",
       "      <td>588</td>\n",
       "      <td>76</td>\n",
       "      <td>85</td>\n",
       "      <td>0.875772</td>\n",
       "      <td>0.878010</td>\n",
       "      <td>0.865506</td>\n",
       "      <td>0.871713</td>\n",
       "      <td>0.875524</td>\n",
       "      <td>0.751379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.287741</td>\n",
       "      <td>558</td>\n",
       "      <td>595</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>0.889660</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.886418</td>\n",
       "      <td>0.889498</td>\n",
       "      <td>0.779167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.333742</td>\n",
       "      <td>546</td>\n",
       "      <td>613</td>\n",
       "      <td>51</td>\n",
       "      <td>86</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.914573</td>\n",
       "      <td>0.863924</td>\n",
       "      <td>0.888527</td>\n",
       "      <td>0.893558</td>\n",
       "      <td>0.789325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>0.360917</td>\n",
       "      <td>551</td>\n",
       "      <td>606</td>\n",
       "      <td>58</td>\n",
       "      <td>81</td>\n",
       "      <td>0.892747</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.871835</td>\n",
       "      <td>0.887994</td>\n",
       "      <td>0.892243</td>\n",
       "      <td>0.785671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>0.431226</td>\n",
       "      <td>550</td>\n",
       "      <td>607</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>0.892747</td>\n",
       "      <td>0.906096</td>\n",
       "      <td>0.870253</td>\n",
       "      <td>0.887813</td>\n",
       "      <td>0.892205</td>\n",
       "      <td>0.785745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400:\n",
      "  Allocated memory: 10228.17 MB\n",
      "  Reserved memory: 19330.00 MB\n",
      "Step 800:\n",
      "  Allocated memory: 10227.91 MB\n",
      "  Reserved memory: 19362.00 MB\n",
      "Step 1200:\n",
      "  Allocated memory: 10227.91 MB\n",
      "  Reserved memory: 19364.00 MB\n",
      "Step 1600:\n",
      "  Allocated memory: 10227.84 MB\n",
      "  Reserved memory: 19364.00 MB\n",
      "Step 2000:\n",
      "  Allocated memory: 10227.94 MB\n",
      "  Reserved memory: 19364.00 MB\n",
      "Step 2400:\n",
      "  Allocated memory: 10227.91 MB\n",
      "  Reserved memory: 19364.00 MB\n"
     ]
    }
   ],
   "source": [
    "# all-params\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: esm2_t33_650M_UR50D-finetuned_toxi/checkpoint-1600\n"
     ]
    }
   ],
   "source": [
    "# best checkpoint after finetuning finished\n",
    "print(f\"Best checkpoint: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4631272256374359,\n",
       " 'eval_TP': 560,\n",
       " 'eval_TN': 583,\n",
       " 'eval_FP': 82,\n",
       " 'eval_FN': 72,\n",
       " 'eval_accuracy': 0.8812644564379337,\n",
       " 'eval_precision': 0.8722741433021807,\n",
       " 'eval_recall': 0.8860759493670886,\n",
       " 'eval_f1-score': 0.8791208791208791,\n",
       " 'eval_AUC': 0.8813838393451984,\n",
       " 'eval_MCC': 0.7625590504037529,\n",
       " 'eval_runtime': 10.2666,\n",
       " 'eval_samples_per_second': 126.332,\n",
       " 'eval_steps_per_second': 7.987,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 1056.3174,\n",
       " 'train_samples_per_second': 29.446,\n",
       " 'train_steps_per_second': 1.84,\n",
       " 'total_flos': 4479845036486976.0,\n",
       " 'train_loss': 0.25209634294235167,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./esm2_t33_650M_UR50D-finetuned_v2/checkpoint-1296/\"\n",
    "# other_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3016924560070038,\n",
       " 'eval_model_preparation_time': 0.0055,\n",
       " 'eval_TP': 534,\n",
       " 'eval_TN': 599,\n",
       " 'eval_FP': 65,\n",
       " 'eval_FN': 98,\n",
       " 'eval_accuracy': 0.8742283950617284,\n",
       " 'eval_precision': 0.8914858096828047,\n",
       " 'eval_recall': 0.8449367088607594,\n",
       " 'eval_f1-score': 0.867587327376117,\n",
       " 'eval_AUC': 0.8735225712978496,\n",
       " 'eval_MCC': 0.7489617263426595,\n",
       " 'eval_runtime': 10.0133,\n",
       " 'eval_samples_per_second': 129.427,\n",
       " 'eval_steps_per_second': 8.089}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate epoch 1 \n",
    "# checkpoint_path_e1 = \"./esm2_t33_650M_UR50D-finetuned_v2/checkpoint-648/\"\n",
    "# model_e1 = AutoModelForSequenceClassification.from_pretrained(checkpoint_path_e1, num_labels=2)\n",
    "# trainer_e1 = Trainer(\n",
    "#     model=model_e1,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# trainer_e1.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b55433bc6b7408dbc8fe66dcb95d9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbef18c0816e4a0c93ff663d44ad6a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/milka1g/esm2_650M_finetuned_toxicity/commit/13f2d12998cf8f67b019aed33b633c24f4ea6d0e', commit_message='Upload tokenizer', commit_description='', oid='13f2d12998cf8f67b019aed33b633c24f4ea6d0e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/milka1g/esm2_650M_finetuned_toxicity', endpoint='https://huggingface.co', repo_type='model', repo_id='milka1g/esm2_650M_finetuned_toxicity'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repo_name = \"milka1g/esm2_650M_finetuned_toxicity\"\n",
    "# checkpoint_path_e3 = \"./esm2_t33_650M_UR50D-finetuned_v2/checkpoint-1944/\"\n",
    "# model_e3 = AutoModelForSequenceClassification.from_pretrained(checkpoint_path_e3, num_labels=2)\n",
    "# model_e3.push_to_hub(repo_name)\n",
    "# tokenizer.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
